= Abstract

== Objectives

During the two months of my internship and under the directives of Luca Berti, my referent, I will be in charge of the computation of solar Shading Masks, View Factors, and Solar Faces. Since this project takes its roots in the 'plan action blablabla emissions CO2', the scale at which the computations have to be done is immense. In fact, we want to be able to compute these at city level, meaning modeling thousands of households and iterate the same computations over them.

My work can be separated into two quasi-independent parts:

**OPTIMIZATION:**

- Code Vectorization using Eigen library
- Use suitable RNG's for random number generation
- Ray-tracing parallelization using OpenMP (which is already needed for the RNG)
- Choose a suitable storage way for the BVH structure (possibly already choose a good one for NVIDIA's OptiX Ray Tracing API)
- Benchmarking for speed and quality purposes

**IMPLEMENTATION:**

- Solar radiation (heat transfer) code integration
- View factors computation (very similar to both others)
- Rewrite the whole code to be used in the OptiX API enabling strong Ray-Tracing capabilities

== Tools

Seeing the scale of this project, numerous tools will have to be used, ranging from benchmarking simple methods to understanding all the underlyings of compilation flags and optimized data structures for BVH representation of complex and large scenes.

For the benchmarks, libraries such as googletest will be used to perform quality benchmarks on our distributions (speed and quality) and on our BVH construction algorithms such as the traversing ones.

For the strict optimization of the code, especially loops and Random Number Generators, hardware-specific and goal-targeted compilation flags will have to be used, described in their respective sections.

To accelerate the entire process and enable fast computations at city scale, the utilization of GPUs plays a crucial role. GPUs (Graphics Processing Units) are highly parallel processors capable of performing large-scale calculations simultaneously. By harnessing the immense computational power of GPUs, complex and demanding tasks can be accelerated, leading to significant improvements in performance. Their capabilities will be harnessed through the use of CUDA, and will further be accelerated using the OptiX Ray Tracing API disposing of optimized intersection tests, ray generation, and more.

Furthermore, the project can benefit from automation, allowing repetitive tasks to be executed on the Gaya cluster. This automation streamlines the process and maximizes efficiency, enabling rapid iterations and scalability.

During the two-month period, the project also aims to leverage GitHub for maintaining project integrity. This involves utilizing integrated submodules for benchmarking different distributions and effectively managing packages of significant sizes, such as Intel's oneAPI MKL library. Additionally, the project aims to enhance skills in using hardware-optimized SIMD (Single Instruction, Multiple Data) techniques, further optimizing performance and computational efficiency.