= Chapter 3 - Becnhmarking and Performance Measurement

== Shell / Bash Scripting and Automation

== Reframe Benchmarking 

=== Definition

=== Utility

== Explicit Benchmarking

== Random Number Generators

When dealing with solar shading masks computation, it's strongly recommended to use the Monte-Carlo technic, namely during the ray-tracing part of the algorithm enabling one to backtrace the paths to find shaded locations due to the environment. But this comes at a cost, since it's mathematically accurate only when tracing multiple rays in random directions, being the foundation of this method.

Since this project is at district or even city scale, the use of a fast Random Number Generators (RNG) is necessary. Despite numerous benchmarks already been done, we decided to compare some of the fastest open-source libraries, all heavily relying on Vectorization, multi-threading or multicore processing, and performance-targeted compiling flags (such as `-03` to `-0fast`). Since Intel processors have specific instructions and extensions (such as Streaming SIMD Extensions, SSE2) and Advanced Vector Extensions (AVX), the MKL advanced Mathematics library naturally benefits of efficient vectorized operations.

=== RNG Speed Benchmark

We have to adopt rigorous coding methods, proving and explaining our reasoning behind every choice made during the internship. In order to settle on a specific method, benchmarks have to be performed.

==== Available Methods

As said, numerous methods are open-source and available to use, built upon different fundamentals. The Xoshiro-Cpp is a lightweight, high-performance random number generator known for its speed and low memory consumption. The Mersenne Twister, on the other hand, is widely used for its long period and good statistical properties. std::random provides a standardized interface for random number generation in C++, offering various generators including Mersenne Twister. Eigen::Rand is a random number generator headers library strongly relying on Eigen's vectorization capabilities, useful for generating random matrices in numerical computations. Intel MKL offers high-performance, vectorized random number generation routines optimized for scientific and numerical computing while `pcg-cpp` is a Permuted Congruential Generator implementation balancing speed and randomness for simulations and gaming.

Below you can see a first execution time comparison, using the `std::chrono` library to measure the time taken by each method to generate 1000000 random numbers. The code was compiled using different usual flags, such as `-O3`, `-Ofast`, `-O2` and `-O1`. 

image::colorcompare.png[Barplot comparing execution time][600]

We can observe that all of the presented methods, except for Eigen's one, are outperforming the standard library.

==== Different Flags

Since Intel's oneAPI MKL library is by far the highest speed RNG, we benchmarked it using more specific flags, such as `march=native` in order to enable all the processor's extensions, and switching between `sequential` and `threaded` modes. The results are shown below:

image::intel.png[width=600]


And below the second speed test done on Gaia, a system composeed of 192 AMD EPYC 7552 48-Core Processors. During this second benchmark, we cycled through most of the different compilation flags combinations, and the results are shown below:

image::AMDspeed.png[600]

link:https://caiorss.github.io/C-Cpp-Notes/compiler-flags-options.html[Here] is a website describing the use-cases of each compilation flag and what they do.

The `-Ofast` and `-O3` flags do the following work for the user:

- Loop unrolling: This optimization involves repeating the body of a loop multiple times, which reduces loop overhead and enables better parallelism.
- Code redundancy elimination: The compiler detects portions of code that are needlessly repeated and replaces them with reused references, reducing the amount of executed code.
- Vectorization: Loops are transformed to use SIMD (Single Instruction, Multiple Data) instructions to perform simultaneous operations on multiple data elements. This effectively exploits the parallel computing capabilities of modern processors.
- Loop fusion: Independent loops are merged into a single loop, reducing overhead and improving performance by reducing the number of iterations.
- Function inlining: Function calls are replaced with the body of the function itself, avoiding the overhead of function calls.
Unused variable elimination: Variables that are not used in the code are detected and removed, reducing memory consumption.
- Arithmetic operation optimization: The compiler can perform mathematical transformations to simplify expressions and reduce the number of necessary operations.

=== RNG Quality Benchmark

=== Summary

==== Integral's precision